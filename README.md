# Awesome-efficiency

Sparsity is an important paradigm to reduce the parameters and accelerate the inference. 
To mitigate LLM inference costs, there are many approaches are proposed, including quantization, pruning, weight sparsification and recent popular mixture of experts. Most recently, researh work have observed that activations in the MLP blocks of LLMs are sparse, which means only a few columns or rows are required in the foward pass. 

| **Paper Title** | **Year** | **Conference/Journal** | **Code** |
| --------------- | :----: | :----: | :----: |
| [Prompt-prompted Mixture of Experts for Efficient LLM Generation](https://arxiv.org/abs/2404.01365v1) | 2024 | Arxiv | no Run|
| [Scalable LLM Math Reasoning Acceleration with Low-rank Distillation](https://arxiv.org/abs/2505.07861) | 2024 | Arxiv | no Run|
| [CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models](https://arxiv.org/pdf/2404.08763) | 2024 | Arxiv | no Run|



## KV cache compression
| **Paper Title** | **Year** | **Conference/Journal** | **Code** |
| --------------- | :----: | :----: | :----: |
| [Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference](https://arxiv.org/abs/2402.09398) | 2024 | Arxiv | no Run|

